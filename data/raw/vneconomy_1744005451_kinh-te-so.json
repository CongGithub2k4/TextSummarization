{
  "category": "kinh-te-so",
  "url": "https://vneconomy.vn/meta-co-the-the-dung-phat-trien-cac-he-thong-ai-neu-qua-rui-ro.htm",
  "source": "vneconomy",
  "scraped_at": "2025-04-07 12:57:31",
  "summary": "Trong một số trường hợp, Meta có thể sẽ không phát hành hệ thống AI có năng lực cao mà họ đã tự phát triển… ",
  "content": "\nCEO của Meta – ông Mark Zuckerberg – từng cam kết sẽ cung cấp trí tuệ nhân tạo tổng quát (AGI) công khai. Đây là công nghệ giúp AI có thể thực hiện bất kỳ nhiệm vụ nào mà con người có thể làm.\nTuy nhiên, tài liệu chính sách mới đây của công ty này cho thấy một số trường hợp Meta có thể sẽ không phát hành hệ thống AI có năng lực cao mà họ đã tự phát triển trong nội bộ, trang TechCrunch đưa tin.\nTài liệu có tên Khung AI tiên phong (Frontier AI Framework) đã xác định hai loại hệ thống AI mà doanh nghiệp này coi là quá rủi ro để phát hành: Hệ thống “rủi ro cao” và hệ thống “rủi ro nghiêm trọng\". \nTheo định nghĩa của Meta, cả hai loại hệ thống trên đều có khả năng hỗ trợ các cuộc tấn công an ninh mạng, hóa học và sinh học. Điểm khác biệt là hệ thống rủi ro nghiêm trọng có thể dẫn đến một “hậu quả thảm khốc không thể giảm thiểu trong bối cảnh triển khai được đề xuất”.  \nTrong khi đó, hệ thống rủi ro cao có thể giúp dễ dàng thực hiện một cuộc tấn công hơn nhưng không đáng tin cậy hoặc không chắc chắn bằng hệ thống rủi ro nghiêm trọng. \nVậy cụ thể là những cuộc tấn công nào? Meta đưa ra một số ví dụ, như “xâm nhập tự động từ đầu đến cuối vào một môi trường doanh nghiệp được bảo vệ theo tiêu chuẩn tốt nhất” và “sự lan rộng của vũ khí sinh học có tác động cao”.\nMeta đánh giá, danh sách các thảm họa tiềm tàng trong tài liệu không mang tính toàn diện, nhưng bao gồm những rủi ro mà công ty này tin là “cấp bách nhất” và có khả năng xảy ra trực tiếp do việc phát hành một hệ thống AI mạnh mẽ. \nĐiều khá đáng ngạc nhiên là theo tài liệu này, Meta phân loại mức độ rủi ro của hệ thống không dựa trên một bài kiểm tra thực nghiệm cụ thể nào, mà dựa trên ý kiến của các nhà nghiên cứu cả nội bộ và bên ngoài – những người được xem xét bởi “các nhà lãnh đạo cấp cao”.\nCâu hỏi đặt ra là tại sao lại như vậy. Meta cho rằng, khoa học đánh giá hiện tại chưa “đủ vững chắc để cung cấp các chỉ số định lượng mang tính quyết định” trong việc xác định mức độ rủi ro của một hệ thống.\nNếu Meta xác định một hệ thống thuộc phân loại rủi ro cao, công ty này sẽ hạn chế quyền truy cập vào hệ thống trong nội bộ và sẽ không phát hành nó cho đến khi triển khai các biện pháp giảm thiểu để “giảm rủi ro xuống mức trung bình.”\nNgược lại, nếu một hệ thống bị coi là rủi ro nghiêm trọng, Meta cho biết sẽ thực hiện các biện pháp bảo vệ an ninh để ngăn chặn việc hệ thống bị đánh cắp và sẽ dừng phát triển cho đến khi có thể làm cho hệ thống trở nên ít nguy hiểm hơn.\nKhung AI tiên phong của Meta sẽ được phát triển cùng với bối cảnh AI đang thay đổi và theo cam kết mà doanh nghiệp đã công bố trước thềm Hội nghị thượng đỉnh hành động AI của Pháp vào tháng này. Động thái này dường như là phản ứng trước những lời chỉ trích về cách tiếp cận của công ty đối với việc phát triển hệ thống.\nMeta đã áp dụng chiến lược công khai công nghệ AI của mình, mặc dù không phải là mã nguồn mở theo định nghĩa thông thường, trái ngược với các công ty như OpenAI lựa chọn bảo vệ hệ thống của họ bằng API.\nKhi công bố khung phát triển mới nhất, Meta cũng có thể muốn đối chiếu chiến lược AI mở của mình với công ty AI Trung Quốc DeepSeek khi tay chơi đến từ Bắc Kinh đã công khai các hệ thống của mình. Nhưng AI của Trung Quốc có ít biện pháp bảo vệ và có thể dễ dàng bị điều khiển để tạo ra các đầu ra độc hại.\n“Chúng tôi tin rằng, bằng cách cân nhắc cả lợi ích và rủi ro khi đưa ra quyết định về cách phát triển và triển khai AI tiên tiến, chúng tôi có thể cung cấp công nghệ đó cho xã hội theo cách bảo toàn lợi ích của công nghệ đó đối với xã hội, đồng thời vẫn duy trì mức độ rủi ro phù hợp”, Meta viết trong tài liệu.\n"
}